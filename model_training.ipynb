{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNTK and TensorFlow models for image classification\n",
    "\n",
    "## Outline\n",
    "- [Preparing an Azure N-Series GPU Deep Learning VM](#prepare)\n",
    "   - [Provision the VM](#provision)\n",
    "   - [Connect to the VM by remote desktop](#rd)\n",
    "   - [Clone/download scripts and supporting files](#repo)\n",
    "   - [Download training set data locally](#trainingset)\n",
    "   - [(Optional) Access the VM remotely via Jupyter Notebook](#jupyter)\n",
    "- [Microsoft Cognitive Toolkit](#cntk)\n",
    "- [TensorFlow](#tensorflow)\n",
    "   - [Training script](#tfscript)\n",
    "   - [Model](#tfmodel)\n",
    "   - [Running the training script](#tfrun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prepare\"></a>\n",
    "## Preparing an Azure N-Series GPU Deep Learning VM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"provision\"></a>\n",
    "### Provision the VM\n",
    "Deploy a \"Deep Learning toolkit for the DSVM\" resource in a region that offers GPU VMs, such as East US. As of this writing (1/19), the DSVM deploys with CNTK 2.0.\n",
    "\n",
    "<a name=\"rd\"></a>\n",
    "### Connect to the VM by remote desktop\n",
    "\n",
    "To use remote desktop, click \"Connect\" on the VM's main pane to download an RDP file. When accessing, make sure that you specify the \"domain\" (VM name) as well as your username, e.g. \"mawahgpudsvm\\mawah\", so that the connection doesn't attempt to use your Microsoft domain.\n",
    "\n",
    "<a name=\"repo\"></a>\n",
    "### Clone/download scripts and supporting files\n",
    "Download the contents of this repo and copy the contents of the `tf` and `cntk` subfolders to appropriate locations. We have used locations on the temporary drive, e.g. `D:\\tf` and `D:\\cntk`.\n",
    "\n",
    "<a name=\"trainingset\"></a>\n",
    "### Download training set data locally\n",
    "During image set preparation, a training image set and descriptive files were created for use with CNTK and TensorFlow. Transfer these files to the GPU VM and store in an appropriate location. (We have used the `D:\\combined\\train_subsample` folder.) If you did not generate a larger training set earlier, you can use the small training set included in this git repo. You may need to regenerate the CNTK map file if image paths have been changed.\n",
    "\n",
    "<a name=\"jupyter\"></a>\n",
    "### (Optional) Access the VM remotely via Jupyter Notebook\n",
    "\n",
    "Follow these steps if you wish to be able to access the notebook server remotely:\n",
    "1. In the [Azure Portal](https://portal.azure.com), navigate to the deployed VM's pane and determine its IP address.\n",
    "1. In the [Azure Portal](https://portal.azure.com), navigate to the deployed VM's Network Security Group's pane and add inbound/outbound rules permitting traffic on port 9999.\n",
    "1. While connected to the VM via remote desktop, launch a command prompt (Windows key + R) and type the following commands:\n",
    "\n",
    "   ```\n",
    "   cd C:\\dsvm\\tools\\setup\n",
    "   JupyterSetPasswordAndStart.cmd\n",
    "   ```\n",
    "\n",
    "   Follow the prompts to set your remote access password.\n",
    "   \n",
    "1. Connect to your VM remotely via Jupyter Notebooks using the IP address you determined earlier and port 9999, e.g. `https://[__.__.__.__]:9999`. The default directory on login will be `C:\\dsvm\\notebooks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cntk\"></a>\n",
    "## Cognitive Toolkit (CNTK)\n",
    "\n",
    "The script below can be used to train a 20-layer ResNet for image classification from scratch. The script is adapted from the [CNTK ResNet/CIFAR10 image classification example](https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Classification/ResNet/Python): if training on a multi-GPU VM, see their example code for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "\n",
    "# Licensed under the MIT license. See LICENSE.md file in the project root\n",
    "# for full license information.\n",
    "# ==============================================================================\n",
    "\n",
    "''' running parameters -- edit as necessary '''\n",
    "data_path  = 'E:\\\\combined\\\\train_subsample2'\n",
    "model_path = 'E:\\\\cntk\\\\models'\n",
    "image_height = 224\n",
    "image_width  = 224\n",
    "num_channels = 3  # RGB\n",
    "num_classes  = 7\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from cntk.initializer import he_normal\n",
    "from cntk.layers import AveragePooling, BatchNormalization, Convolution, Dense\n",
    "from cntk.utils import *\n",
    "from cntk.ops import input_variable, cross_entropy_with_softmax, classification_error, element_times, relu\n",
    "from cntk.io import MinibatchSource, ImageDeserializer, StreamDef, StreamDefs\n",
    "from cntk import Trainer, cntk_py\n",
    "from cntk.learner import momentum_sgd, learning_rate_schedule, momentum_as_time_constant_schedule, UnitType\n",
    "from _cntk_py import set_computation_network_trace_level\n",
    "\n",
    "# Helper functions for ResNet construction\n",
    "def conv_bn(input, filter_size, num_filters, strides=(1,1), init=he_normal()):\n",
    "    c = Convolution(filter_size, num_filters, activation=None, init=init, pad=True, strides=strides, bias=False)(input)\n",
    "    r = BatchNormalization(map_rank=1, normalization_time_constant=4096, use_cntk_engine=False)(c)\n",
    "    return r\n",
    "\n",
    "def conv_bn_relu(input, filter_size, num_filters, strides=(1,1), init=he_normal()):\n",
    "    r = conv_bn(input, filter_size, num_filters, strides, init) \n",
    "    return relu(r)\n",
    "\n",
    "def resnet_basic(input, num_filters):\n",
    "    c1 = conv_bn_relu(input, (3,3), num_filters)\n",
    "    c2 = conv_bn(c1, (3,3), num_filters)\n",
    "    p  = c2 + input\n",
    "    return relu(p)\n",
    "\n",
    "def resnet_basic_inc(input, num_filters, strides=(2,2)):\n",
    "    c1 = conv_bn_relu(input, (3,3), num_filters, strides)\n",
    "    c2 = conv_bn(c1, (3,3), num_filters)\n",
    "    s  = conv_bn(input, (1,1), num_filters, strides)\n",
    "    p  = c2 + s\n",
    "    return relu(p)\n",
    "\n",
    "def resnet_basic_stack(input, num_stack_layers, num_filters): \n",
    "    assert (num_stack_layers >= 0)\n",
    "    l = input \n",
    "    for _ in range(num_stack_layers): \n",
    "        l = resnet_basic(l, num_filters)\n",
    "    return l \n",
    "\n",
    "def create_model(input, num_stack_layers, num_classes):\n",
    "    c_map = [16, 32, 64]\n",
    "    conv = conv_bn_relu(input, (3,3), c_map[0])\n",
    "    r1 = resnet_basic_stack(conv, num_stack_layers, c_map[0])\n",
    "    r2_1 = resnet_basic_inc(r1, c_map[1])\n",
    "    r2_2 = resnet_basic_stack(r2_1, num_stack_layers-1, c_map[1])\n",
    "    r3_1 = resnet_basic_inc(r2_2, c_map[2])\n",
    "    r3_2 = resnet_basic_stack(r3_1, num_stack_layers-1, c_map[2])\n",
    "    pool = AveragePooling(filter_shape=(8,8))(r3_2) \n",
    "    z = Dense(num_classes)(pool)\n",
    "    return z\n",
    "\n",
    "# Function for accessing and preprocessing the images\n",
    "def create_reader(map_file):\n",
    "    if not os.path.exists(map_file):\n",
    "        raise RuntimeError(\"File '{}' does not exist\".format(map_file))\n",
    "\n",
    "    # transformation pipeline for the features has jitter/crop only when training\n",
    "    transforms = [ImageDeserializer.scale(width=image_width,\n",
    "                                          height=image_height,\n",
    "                                          channels=num_channels,\n",
    "                                          interpolations='linear')]\n",
    "    return MinibatchSource(ImageDeserializer(map_file, StreamDefs(\n",
    "        features = StreamDef(field='image', transforms=transforms), # first column in map file is referred to as 'image'\n",
    "        labels   = StreamDef(field='label', shape=num_classes))))   # and second as 'label'\n",
    "\n",
    "# Function for coordinating training\n",
    "def train(reader_train, epoch_size, max_epochs, model_location=None):\n",
    "    set_computation_network_trace_level(0)\n",
    "    input_var = input_variable((num_channels, image_height, image_width))\n",
    "    label_var = input_variable((num_classes))\n",
    "\n",
    "    z = create_model(input_var, 8, num_classes)\n",
    "    lr_per_mb = [0.001]+[0.01]*80+[0.001]*40+[0.0001]\n",
    "\n",
    "    # loss and metric\n",
    "    ce = cross_entropy_with_softmax(z, label_var)\n",
    "    pe = classification_error(z, label_var)\n",
    "\n",
    "    minibatch_size = 16\n",
    "    momentum_time_constant = -minibatch_size/np.log(0.9)\n",
    "    l2_reg_weight = 0.0001\n",
    "    \n",
    "    lr_per_sample = [lr/minibatch_size for lr in lr_per_mb]\n",
    "    lr_schedule = learning_rate_schedule(lr_per_sample, epoch_size=epoch_size, unit=UnitType.sample)\n",
    "    mm_schedule = momentum_as_time_constant_schedule(momentum_time_constant)\n",
    "    \n",
    "    learner = momentum_sgd(z.parameters, lr_schedule, mm_schedule,\n",
    "                           l2_regularization_weight = l2_reg_weight,\n",
    "                           unit_gain=True)\n",
    "    trainer = Trainer(z, ce, pe, learner)\n",
    "    if model_location is not None:\n",
    "        trainer.restore_from_checkpoint(model_location)\n",
    "\n",
    "    input_map = {input_var: reader_train.streams.features,\n",
    "                 label_var: reader_train.streams.labels}\n",
    "\n",
    "    log_number_of_parameters(z) ; print()\n",
    "    progress_printer = ProgressPrinter(tag='Training')\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        sample_count = 0\n",
    "        while sample_count < epoch_size:\n",
    "            data = reader_train.next_minibatch(min(minibatch_size, epoch_size-sample_count), input_map=input_map)\n",
    "            trainer.train_minibatch(data)\n",
    "            sample_count += trainer.previous_minibatch_sample_count\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True)\n",
    "        progress_printer.epoch_summary(with_metric=True)\n",
    "        z.save_model(os.path.join(model_path, 'resnet50_{}.dnn'.format(epoch)))\n",
    "    return\n",
    "\n",
    "# Create data reader and run training\n",
    "reader_train = create_reader(os.path.join(data_path, 'map.txt'))\n",
    "train(reader_train, network_name, epoch_size=1000, max_epochs=160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details of the model evaluation process, please see the socring notebook in the [Embarrassingly Parallel Image Classification](https://github.com/Azure/Embarrassingly-Parallel-Image-Classification) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tensorflow\"></a>\n",
    "## Tensorflow\n",
    "\n",
    "<a name=\"tfscript\"></a>\n",
    "### Training script\n",
    "\n",
    "We made use of the [`tf-slim` API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) for Tensorflow, which provides pre-trained ResNet models and helpful scripts for retraining and scoring. During training set preparation, we converted raw PNG images to the [TFRecords](https://www.tensorflow.org/how_tos/reading_data/#file_formats) files that those scripts expect as input. (Our evaluation set images will be scored on Spark without conversion to TFRecord format.)\n",
    "\n",
    "Our training script is a modified version of `train_image_classifier.py` from the [Tensorflow models repo's slim subdirectory](https://github.com/tensorflow/models/tree/master/slim). Changes have also been made to some of that script's dependencies. We recommend that you clone this repo and transfer the `tf` subfolder, including dependencies, to a suitable location, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repo_dir = 'D:\\\\tf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tfmodel\"></a>\n",
    "### Model\n",
    "\n",
    "We will retrain the logits of a 152-layer ResNet pretrained on ImageNet. This model is highlighted in the [Tensorflow models repo's slim subdirectory](https://github.com/tensorflow/models/tree/master/slim). The pretrained model can be obtained and unpacked with the code snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "urllib.request.urlretrieve('http://download.tensorflow.org/models/resnet_v1_152_2016_08_28.tar.gz',\n",
    "                           os.path.join(repo_dir, 'resnet_v1_152_2016_08_28.tar.gz'))\n",
    "with tarfile.open(os.path.join(repo_dir, 'resnet_v1_152_2016_08_28.tar.gz'), 'r:gz') as f:\n",
    "    f.extractall(path=repo_dir)\n",
    "os.remove(os.path.join(repo_dir, 'resnet_v1_152_2016_08_28.tar.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tfrun\"></a>\n",
    "### Running the training script\n",
    "\n",
    "We recommend that you run the training script from an Anaconda prompt. The code cell below will help you generate the appropriate command based on your file locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path where retrained model and logs will be saved during training\n",
    "train_dir = os.path.join(repo_dir, 'models')\n",
    "if not os.path.exists(train_dir):\n",
    "    os.makedirs(train_dir)\n",
    "    \n",
    "# location of the unpacked pretrained model\n",
    "checkpoint_path = os.path.join(repo_dir, 'resnet_v1_152.ckpt')\n",
    "\n",
    "# Location of the TFRecords and other files generated during image set preparation\n",
    "image_dir = 'D:\\\\combined\\\\train_subsample'\n",
    "\n",
    "command = '''activate py35\n",
    "python {0} --train_dir={1} --dataset_name=aerial --dataset_split_name=train --dataset_dir={2} --checkpoint_path={3}\n",
    "'''.format(os.path.join(repo_dir, 'retrain.py'),\n",
    "           train_dir,\n",
    "           dataset_dir,\n",
    "           checkpoint_path)\n",
    "\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details of the model evaluation process, please see the socring notebook in the [Embarrassingly Parallel Image Classification](https://github.com/Azure/Embarrassingly-Parallel-Image-Classification) repository."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
