{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of a labeled set of aerial images from public data sources\n",
    "\n",
    "This notebook illustrates how a set of aerial images, labeled by land use classification, was generated from freely available U.S. datasets. The image sets generated in this notebook were used to train a DNN for image classification and evaluate its performance. For more detail, please see the rest of the [Embarrassingly Parallel Image Classification](https://github.com/Azure/Embarrassingly-Parallel-Image-Classification) repository.\n",
    "\n",
    "This walkthrough will demonstrate the technique using source data from Middlesex County, MA, the home of Microsoft's New England Research & Development (NERD) Center. The method was applied to many U.S. counties in parallel to generate the full training and evaluation sets.\n",
    "\n",
    "<img src=\"./img/data_overview/middlesex_ma.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- [Source data](#sourcedata)\n",
    "   - [National Agriculture Imagery Program](#naip)\n",
    "   - [National Land Cover Database](#nlcd)\n",
    "- [Preparing an Azure Data Science Virtual Machine for image extraction](#dsvm)\n",
    "   - [Provisioning and accessing the DSVM](#provision)\n",
    "   - [Installing Python packages and supporting software](#install)\n",
    "   - [Downloading input data locally](#download)\n",
    "- [Producing image sets for training and evaluation](#production)\n",
    "   - [Overview of the approach](#overview)\n",
    "   - [Converting NAIP images from MrSID to GeoTIFF](#conversion)\n",
    "   - [Add tiling for fast data extraction](#tiling)\n",
    "   - [Find a lat-lon bounding box for the region](#box)\n",
    "   - [Get the approximate dimensions of the bounding box in meters](#meters)\n",
    "   - [Finding tiles with consistent land use class](#consistent)\n",
    "   - [Extract and save images for tiles of interest](#extraction)\n",
    "   - [Combining similar labels](#combine)\n",
    "   - [Dataset partitioning](#partition)\n",
    "- [Dataset preparation for deep learning](#prep)\n",
    "   - [Cognitive Toolkit (CNTK)](#cntk)\n",
    "   - [TensorFlow](#tf)\n",
    "- [Data Transfer](#transfer)\n",
    "- [Next Steps](#nextsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"sourcedata\"></a>\n",
    "## Source data\n",
    "\n",
    "<a name=\"naip\"></a>\n",
    "### National Agriculture Imagery Program\n",
    "\n",
    "The US [National Agriculture Imagery Program](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/index) is run by the [US Department of Agriculture](https://www.usda.gov/)'s [Farm Service Agency](https://www.fsa.usda.gov/). NAIP images are provided within one month of image collection, in natural (RGB) color with 1-meter ground sample distance. In this tutorial, we use Compressed County Mosaics obtained from the [Geospatial Data Gateway](https://gdg.sc.egov.usda.gov/); you can download the 2016 Middlesex County, MA directly from [our mirror](https://mawahstorage.blob.core.windows.net/aerialimageclassification/naipsample/ortho_imagery_NAIPM16_ma017_3344056_01.zip) if you prefer.\n",
    "\n",
    "A zoomed-out view of the 2016 NAIP data covering Middlesex County, MA is shown below (Cambridge and Boston at south end of east edge):\n",
    "\n",
    "<img src=\"./img/data_overview/mediumnaip_white.png\" width=\"300 px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"nlcd\"></a>\n",
    "### National Land Cover Database\n",
    "\n",
    "The US [National Land Cover Database](https://www.mrlc.gov/nlcd2011.php) (NLCD), maintained by the [Multi-Resolution Land Characteristic Consortium](https://www.mrlc.gov/), provides land use labels at 30-meter spatial resolution for the United States. The [sixteen land use classes](https://www.mrlc.gov/nlcd11_leg.php) in the NLCD are organized hierarchically into major (Developed, Forested, Planted/Cultivated, &c.) and minor (e.g. Deciduous, Evergreen, or Mixed Forest) categories. Classifications are based largely on seasonally-collected satellite imagery (including data collected outside the visual spectrum). Because of the extensive post-processing required to prepare this dataset, new versions are published approximately every five years, often with a multi-year delay between data collection and publication. For this project, we use the [NLCD 2011 Land Cover](https://www.mrlc.gov/nlcd11_data.php) dataset.\n",
    "\n",
    "The colorized NLCD data for Middlesex County, MA are shown below. Developed land is shown in shades of red, water/wetlands in blue/cyan, forested land in green, and cultivated land in magenta.\n",
    "\n",
    "<img src=\"./img/data_overview/mediumnlcd.png\" width=\"300 px\" />\n",
    "\n",
    "For more information on the NLCD, please see the following publication:\n",
    "\n",
    "Homer, C.G., Dewitz, J.A., Yang, L., Jin, S., Danielson, P., Xian, G., Coulston, J., Herold, N.D., Wickham, J.D., and Megown, K., 2015, [Completion of the 2011 National Land Cover Database for the conterminous United States - Representing a decade of land cover change information](http://bit.ly/1K7WjO3). Photogrammetric Engineering and Remote Sensing, v. 81, no. 5, p. 345-354 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"dsvm\"></a>\n",
    "## Preparing an Azure Data Science Virtual Machine for image extraction\n",
    "\n",
    "For reproducibility and brevity, this walkthrough describes how to set up an [Azure Data Science Virtual Machine](https://azure.microsoft.com/en-us/marketplace/partners/microsoft-ads/standard-data-science-vm/) for image extraction. It is very likely that the steps below can be adapted for your own computer.\n",
    "\n",
    "<a name=\"provision\"></a>\n",
    "### Provisioning and accessing the DSVM\n",
    "\n",
    "To provision a Data Science Virtual Machine, you will need an [Azure account](https://azure.microsoft.com/en-us/free/). Click the \"Deploy to Azure\" button to begin the process:\n",
    "\n",
    "<a href=\"https://azuredeploy.net/?repository=https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Data-Science-Virtual-Machine/Windows\">![Deploy to Azure](https://camo.githubusercontent.com/a941ea1d057c4efc2dcc0a680f43c97728ec0bd8/687474703a2f2f617a7572656465706c6f792e6e65742f6465706c6f79627574746f6e2e737667)</a>\n",
    "\n",
    "After logging in, you can select a name for your VM, login information, and [VM size](https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-windows-sizes#a-series). We chose an \"A4\" VM because of the large temporary hard disk space included. (Aerial imagery data for some counties can take up >100 GB when decompressed and converted to GeoTIFF format.) If your hard disk requirements are smaller, you may prefer a VM with a solid-state drive for faster file I/O. Confirm your selections to begin deployment.\n",
    "\n",
    "After deployment concludes, navigate to the VM's overview pane in [Azure Portal](https://portal.azure.com) by e.g. searching for the VM or resource name. Click the \"Connect\" button along the top of the pane to download a Remote Desktop connection file. Once the download completes, double-click the file to connect to your VM.\n",
    "\n",
    "Note that your VM will continue to run even after you disconnect from Remote Desktop. You can stop or delete the VM at any time from [Azure Portal](https://portal.azure.com).\n",
    "\n",
    "<a name=\"install\"></a>\n",
    "### Installing Python packages and supporting software\n",
    "\n",
    "#### LizardTech's GeoExpress Command Line Applications\n",
    "NAIP imagery provided in [MrSID](https://en.wikipedia.org/wiki/MrSID) format: we will convert them to GeoTIFF format using LizardTech's free [GeoExpress Command Line Applications](https://www.lizardtech.com/gis-tools/tools-and-utilities). After downloading and decompressing the archive on your VM, copy the files to a permanent directory of your choice. Add the path to the binary file `mrsidgeodecode` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mrsidgeodecode_path = 'C:\\\\Program Files\\\\LizardTech\\\\GeoExpressCLUtils-9.5.0.4326-win64\\\\bin\\\\mrsidgeodecode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python packages\n",
    "To read the resulting GeoTIFF, we will use the GDAL package from within Python. As of this writing, Christoph Gohlke's [Unofficial Windows Binaries for Python Extension Packages](http://www.lfd.uci.edu/~gohlke/pythonlibs/) provide the easiest means to install the GDAL package, binaries, and library. We downloaded the Python 3.5 wheels for the following packages from Christoph's site for this walkthrough:\n",
    "- [GDAL](http://www.lfd.uci.edu/~gohlke/pythonlibs/#gdal)\n",
    "- [Basemap](http://www.lfd.uci.edu/~gohlke/pythonlibs/#basemap)\n",
    "\n",
    "To install these wheels:\n",
    "1. Launch an Anaconda Prompt\n",
    "1. Activate the Python 3.5 environment pre-installed on your DSVM using the following command:\n",
    "\n",
    "  `activate py35`<br/><br/>\n",
    "  \n",
    "1. Install each wheel (`[wheel filename]`) with the following command:\n",
    "\n",
    "  `pip install [wheel filename]`\n",
    "\n",
    "<a name=\"download\"></a>\n",
    "### Downloading input data locally\n",
    "\n",
    "#### National Land Cover Database\n",
    "\n",
    "[Download](https://www.mrlc.gov/nlcd11_data.php) the compressed 2011 National Land Cover Database file (1.1 GB) and extract its contents to a folder of your choice (we used D:\\nlcd). Store the filepath with a `.img` extension by editing and executing the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlcd_filepath = 'C:\\\\Users\\\\mawah\\\\Documents\\\\landcover\\\\nlcd_2011_landcover_2011_edition_2014_10_10\\\\nlcd_2011_landcover_2011_edition_2014_10_10.img'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### National Agriculture Imagery Program\n",
    "\n",
    "As of this writing (1/31/2017), an unplanned outage complicates access to NAIP Compressed County Mosaics via the [Geospatial Data Gateway](https://gdg.sc.egov.usda.gov/). We have mirrored [a sample file](https://mawahstorage.blob.core.windows.net/aerialimageclassification/naipsample/ortho_imagery_NAIPM16_ma017_3344056_01.zip) (Middlesex County, MA 2016 data) for your convenience in following this tutorial. (You can make additional county requests using the email address provided on the gateway.) Download and decompress the file, storing the folder path in the following variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "naip_dir = 'C:\\\\Users\\\\mawah\\\\Documents\\\\landcover\\\\ortho_imagery_NAIPM16_ma025_3364482_01\\\\ortho_imagery'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"production\"></a>\n",
    "## Producing image sets for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"overview\"></a>\n",
    "### Overview of the approach\n",
    "\n",
    "Our goal is to extract many smaller aerial images (\"tiles\") from the NAIP GeoTIFF. Each tile will be labeled with a land use classification determined from the NLCD data.\n",
    "\n",
    "#### Choosing tile size\n",
    "ResNet DNNs are traditionally trained with 224 px x 224 px images: this sets our minimum tile size at 224 meters x 224 meters (because our aerial imagery has approx. 1 meter resolution). A sample tile with these dimensions is shown below at full size.\n",
    "\n",
    "<img src=\"./img/extraction/sample_tile.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposing candidate tiles\n",
    "\n",
    "We divided the Middlesex County, MA region along a rectangular grid of candidate tiles.\n",
    "\n",
    "<img src=\"./img/extraction/common_naip_tiled.png\" />\n",
    "\n",
    "Many tiles from this grid were not retained in our image set because they could not be assigned a land use class label (see next subsection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning land use classes to tiles\n",
    "\n",
    "It is straightforward to label a tile if the land use classification is homogeneous throughout the tile. Unfortunately, it is very common for a 224 m x 224 m region chosen at random to include multiple different land uses. To illustrate this problem, we overlaid the NLCD and NAIP data in a region centered on the [Boston Common](https://binged.it/2kTjwOe) (NAIP-only image shown above):\n",
    "\n",
    "<img src=\"./img/extraction/common_tiled_only.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that too few tiles would be extracted if we required that each tile have a single land use. We developed an arbitrary, more lenient criterion for tile inclusion by sampling the land use class at nine regularly-spaced points within each tile, as depicted below. If all nine points sampled had the same land use class, we retained the tile for our image set and assigned the land use class as a label.\n",
    "\n",
    "<img src=\"./img/extraction/common_points.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conversion\"></a>\n",
    "### Converting NAIP images from MrSID to GeoTIFF\n",
    "\n",
    "We used [LizardTech's GeoExpress Command Line Application](LizardTech's GeoExpress Command Line Application) `mrsidgeodecode` to convert the aerial imagery data from MrSID format to GeoTIFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "filename_base = None\n",
    "for filename in os.listdir(naip_dir):\n",
    "    if filename.endswith('.sid'):\n",
    "        filename_base = filename.split('.sid')[0]\n",
    "        break\n",
    "if filename_base == None:\n",
    "    raise Exception(\"Couldn't find the MrSID file in {}\".format(naip_dir))\n",
    "    \n",
    "geodecode_command = [mrsidgeodecode_path, '-wf',\n",
    "                     '-i', os.path.join(naip_dir, '{}.sid'.format(filename_base)),\n",
    "                     '-o', os.path.join(naip_dir, '{}.tif'.format(filename_base))]\n",
    "results = shutil.run(geodecode_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tiling\"></a>\n",
    "### Add tiling for fast data extraction\n",
    "\n",
    "To speed up image extraction images from arbitrary regions of the GeoTIFF, we add tiling to the image. Since we must write the tiled image before deleting the untiled version, we effectively double our disk space usage at this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdal_command = ['C:\\\\Anaconda\\\\envs\\\\py35\\\\Lib\\\\site-packages\\\\osgeo\\\\gdal_translate',\n",
    "            '-co', 'TILED=YES',\n",
    "            os.path.join(naip_dir, '{}.tif'.format(filename_base)),\n",
    "            os.path.join(naip_dir, '{}_tiled.tif'.format(filename_base))]\n",
    "results = run(gdal_command)\n",
    "os.remove(os.path.join(naip_dir, '{}.tif'.format(filename_base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "naip_filepath = os.path.join(naip_dir, '{}_tiled.tif'.format(filename_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"box\"></a>\n",
    "### Find a lat-lon bounding box for the region\n",
    "\n",
    "We now find the lat-lon bounding box of the NAIP GeoTIFF. Some locations inside this bounding box will not have any available data, but the box is still useful for first-order estimation of the usable region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "from gdalconst import *\n",
    "import osr\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from collections import namedtuple\n",
    "\n",
    "LatLonBounds = namedtuple('LatLonBounds', ['llcrnrlat', 'llcrnrlon', 'urcrnrlat', 'urcrnrlon'])\n",
    "\n",
    "def get_bounding_box(naip_filepath):\n",
    "    ''' Finds a bounding box for the NAIP GeoTIFF in lat/lon '''\n",
    "    naip_image = gdal.Open(naip_filepath, GA_ReadOnly)\n",
    "    naip_proj = osr.SpatialReference()\n",
    "    naip_proj.ImportFromWkt(naip_image.GetProjection())\n",
    "    naip_ulcrnrx, naip_xstep, _, naip_ulcrnry, _, naip_ystep = naip_image.GetGeoTransform()\n",
    "\n",
    "    world_map = Basemap(lat_0 = 0,\n",
    "                        lon_0 = 0,\n",
    "                        llcrnrlat=-90, urcrnrlat=90,\n",
    "                        llcrnrlon=-180, urcrnrlon=180,\n",
    "                        resolution='c', projection='stere')\n",
    "    world_proj = osr.SpatialReference()\n",
    "    world_proj.ImportFromProj4(world_map.proj4string)\n",
    "    ct_to_world = osr.CoordinateTransformation(naip_proj, world_proj)\n",
    "    \n",
    "    lats = []\n",
    "    lons = []\n",
    "    for corner_x, corner_y in [(naip_ulcrnrx, naip_ulcrnry),\n",
    "                               (naip_ulcrnrx, naip_ulcrnry + naip_image.RasterYSize * naip_ystep),\n",
    "                               (naip_ulcrnrx + naip_image.RasterXSize * naip_xstep,\n",
    "                                naip_ulcrnry + naip_image.RasterYSize * naip_ystep),\n",
    "                               (naip_ulcrnrx + naip_image.RasterXSize * naip_xstep, naip_ulcrnry)]:\n",
    "        xpos, ypos, _ = ct_to_world.TransformPoint(corner_x, corner_y)\n",
    "        lon, lat = world_map(xpos, ypos, inverse=True)\n",
    "        lats.append(lat)\n",
    "        lons.append(lon)\n",
    "\n",
    "    return(LatLonBounds(llcrnrlat=min(lats),\n",
    "                        llcrnrlon=min(lons),\n",
    "                        urcrnrlat=max(lats),\n",
    "                        urcrnrlon=max(lons)))\n",
    "\n",
    "region_bounds = get_bounding_box(naip_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"meters\"></a>\n",
    "### Get the approximate dimensions of the bounding box in meters\n",
    "\n",
    "Here we approximate the width/height of the bounding box in meters, using the fact that latitude does not change substantially on the county scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RegionSize = namedtuple('RegionSize', ['width', 'height'])  # in meters!\n",
    "import numpy as np\n",
    "\n",
    "def get_approx_region_size(region_bounds):\n",
    "    ''' Returns the region width (at mid-lat) and height in meters'''\n",
    "    mid_lat_radians = (region_bounds.llcrnrlat + region_bounds.urcrnrlat) * \\\n",
    "                      (np.pi / 360)\n",
    "    earth_circumference = 6.371E6 * 2 * np.pi # in meters\n",
    "    region_middle_width_meters = (region_bounds.urcrnrlon - region_bounds.llcrnrlon) * \\\n",
    "                                 earth_circumference * np.cos(mid_lat_radians) / (360)\n",
    "    region_height_meters = (region_bounds.urcrnrlat - region_bounds.llcrnrlat) * \\\n",
    "                           earth_circumference / (360)\n",
    "    return(RegionSize(region_middle_width_meters, region_height_meters))\n",
    "\n",
    "approx_region_size = get_approx_region_size(region_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"consistent\"></a>\n",
    "### Finding tiles with consistent land use class\n",
    "\n",
    "#### Define helper functions\n",
    "\n",
    "The function below defines a point labeler that we will use for tile selection. At the same time, we define an image extractor that we will use later in the tutorial. (Both functions require a coordinate transform from lat-lon to GeoTIFF-specific coordinates; defining them together keeps things succinct.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def create_helper_functions(region_bounds, nlcd_filepath, naip_filepath):\n",
    "    ''' Makes helper functions to label points (NLCD) and extract tiles (NAIP) '''\n",
    "    nlcd_image = gdal.Open(nlcd_filepath, GA_ReadOnly)\n",
    "    nlcd_proj = osr.SpatialReference()\n",
    "    nlcd_proj.ImportFromWkt(nlcd_image.GetProjection())\n",
    "    nlcd_ulcrnrx, nlcd_xstep, _, nlcd_ulcrnry, _, nlcd_ystep = nlcd_image.GetGeoTransform()\n",
    "    \n",
    "    naip_image = gdal.Open(naip_filepath, GA_ReadOnly)\n",
    "    naip_proj = osr.SpatialReference()\n",
    "    naip_proj.ImportFromWkt(naip_image.GetProjection())\n",
    "    naip_ulcrnrx, naip_xstep, _, naip_ulcrnry, _, naip_ystep = naip_image.GetGeoTransform()\n",
    "    \n",
    "    region_map = Basemap(lat_0 = (region_bounds.llcrnrlat + region_bounds.urcrnrlat)/2,\n",
    "                         lon_0 = (region_bounds.llcrnrlon + region_bounds.urcrnrlon)/2,\n",
    "                         llcrnrlat=region_bounds.llcrnrlat,\n",
    "                         llcrnrlon=region_bounds.llcrnrlon,\n",
    "                         urcrnrlat=region_bounds.urcrnrlat,\n",
    "                         urcrnrlon=region_bounds.urcrnrlon,\n",
    "                         resolution='c',\n",
    "                         projection='stere')\n",
    "    \n",
    "    region_proj = osr.SpatialReference()\n",
    "    region_proj.ImportFromProj4(region_map.proj4string)\n",
    "    ct_to_nlcd = osr.CoordinateTransformation(region_proj, nlcd_proj)\n",
    "    ct_to_naip = osr.CoordinateTransformation(region_proj, naip_proj)\n",
    "\n",
    "    def get_nlcd_label(point):\n",
    "        ''' Project lat/lon point to NLCD GeoTIFF; return label of that point '''\n",
    "        basemap_coords = region_map(point.lon, point.lat)  # NB unusual argument order\n",
    "        x, y, _ = [int(i) for i in ct_to_nlcd.TransformPoint(*basemap_coords)]\n",
    "        xoff = int(round((x - nlcd_ulcrnrx) / nlcd_xstep))\n",
    "        yoff = int(round((y - nlcd_ulcrnry) / nlcd_ystep))\n",
    "        label = int(nlcd_image.ReadAsArray(xoff=xoff, yoff=yoff, xsize=1, ysize=1))\n",
    "        return(label)\n",
    "        \n",
    "    def get_naip_tile(tile_bounds, tile_size):\n",
    "        ''' Check that tile lies within county bounds; if so, extract its image '''\n",
    "        \n",
    "        # Transform tile bounds in lat/lon to NAIP projection coordinates\n",
    "        xmax, ymax = region_map(tile_bounds.urcrnrlon, tile_bounds.urcrnrlat)\n",
    "        xmin, ymin = region_map(tile_bounds.llcrnrlon, tile_bounds.llcrnrlat)\n",
    "        xstep = (xmax - xmin) / tile_size.width\n",
    "        ystep = (ymax - ymin) / tile_size.height\n",
    "\n",
    "        grid = np.mgrid[xmin:xmax:tile_size.width * 1j, ymin:ymax:tile_size.height * 1j]\n",
    "        shape = grid[0, :, :].shape\n",
    "        size = grid[0, :, :].size\n",
    "        xy_target = np.array(ct_to_naip.TransformPoints(grid.reshape(2, size).T))\n",
    "        xx = xy_target[:,0].reshape(shape)\n",
    "        yy = xy_target[:,1].reshape(shape)\n",
    "        \n",
    "        # Extract rectangle from NAIP GeoTIFF containing superset of needed points\n",
    "        xoff = int(round((xx.min() - naip_ulcrnrx) / naip_xstep))\n",
    "        yoff = int(round((yy.max() - naip_ulcrnry) / naip_ystep))\n",
    "        xsize_to_use = int(np.ceil((xx.max() - xx.min())/np.abs(naip_xstep))) + 1\n",
    "        ysize_to_use = int(np.ceil((yy.max() - yy.min())/np.abs(naip_ystep))) + 1\n",
    "        data = naip_image.ReadAsArray(xoff=xoff,\n",
    "                                      yoff=yoff,\n",
    "                                      xsize=xsize_to_use,\n",
    "                                      ysize=ysize_to_use)        \n",
    "        # Map the pixels of interest in NAIP GeoTIFF to the tile (might involve rotation or scaling)\n",
    "        image = np.zeros((xx.shape[1], xx.shape[0], 3)).astype(int)  # rows are height, cols are width, third dim is color\n",
    "        \n",
    "        try:\n",
    "            for i in range(xx.shape[0]):\n",
    "                for j in range(xx.shape[1]):\n",
    "                    x_idx = int(round((xx[i,j] - naip_ulcrnrx) / naip_xstep)) - xoff\n",
    "                    y_idx = int(round((yy[i,j] - naip_ulcrnry) / naip_ystep)) - yoff\n",
    "                    image[xx.shape[1] - j - 1, i, :] = data[:, y_idx, x_idx]\n",
    "        except TypeError as e:\n",
    "            # The following can occur if our pixel superset request exceeds the GeoTIFF's bounds\n",
    "            return(None)\n",
    "        \n",
    "        if np.sum(image.sum(axis=2) == 0) > 10: # too many nodata pixels\n",
    "            return None\n",
    "        \n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "        return(image)\n",
    "    \n",
    "    return(get_nlcd_label, get_naip_tile)\n",
    "get_nlcd_label, get_naip_tile = create_helper_functions(region_bounds, nlcd_filepath, naip_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tile selection\n",
    "\n",
    "We form a rectangular grid of candidate tiles spanning as much of the (approximated-as-rectangular) bounding box as possible. Tiles that have \"reasonably homogeneous\" land use classification are retained for image extraction in the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LatLonPosition = namedtuple('LatLonPosition', ['lat', 'lon'])\n",
    "Tile = namedtuple('Tile', ['bounds', 'label'])\n",
    "\n",
    "def find_tiles_with_consistent_labels(region_bounds, region_size, tile_size):\n",
    "    ''' Find tiles for which nine grid points all have the same label '''\n",
    "    tiles_wide = int(np.floor(region_size.width / tile_size.width))\n",
    "    tiles_tall = int(np.floor(region_size.height / tile_size.height))\n",
    "    tile_width = (region_bounds.urcrnrlon - region_bounds.llcrnrlon) / (region_size.width / tile_size.width)\n",
    "    tile_height = (region_bounds.urcrnrlat - region_bounds.llcrnrlat) / (region_size.height / tile_size.height)\n",
    "    \n",
    "    current_lat = region_bounds.llcrnrlat\n",
    "    current_lon = region_bounds.llcrnrlon\n",
    "    \n",
    "    tiles_to_use = []\n",
    "    for i in range(tiles_tall):\n",
    "        for j in range(tiles_wide):\n",
    "            try:\n",
    "                labels = []\n",
    "                for k in range(3):\n",
    "                    for ell in range(3):\n",
    "                        labels.append(get_nlcd_label(LatLonPosition(lat=current_lat + tile_width * (1 + 2*k) / 6,\n",
    "                                                                    lon=current_lon + tile_height * (1 + 2*ell) / 6)))\n",
    "                num_matching = np.sum(np.array(labels) == labels[4])\n",
    "                if (num_matching == 9) and (labels[4] > 10):\n",
    "                    bounds = LatLonBounds(llcrnrlat=current_lat,\n",
    "                                          llcrnrlon=current_lon,\n",
    "                                          urcrnrlat=current_lat + tile_height,\n",
    "                                          urcrnrlon=current_lon + tile_width)\n",
    "                    tiles_to_use.append(Tile(bounds=bounds,\n",
    "                                             label=labels[4]))\n",
    "            except KeyError:\n",
    "                pass\n",
    "            current_lon += tile_width\n",
    "        current_lon = region_bounds.llcrnrlon\n",
    "        current_lat += tile_height\n",
    "    return(tiles_to_use)\n",
    "\n",
    "tile_size = RegionSize(224, 224)\n",
    "tiles = find_tiles_with_consistent_labels(region_bounds, approx_region_size, tile_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"extraction\"></a>\n",
    "### Extract and save images for tiles of interest\n",
    "\n",
    "The most time-consuming step is the extraction of images from the GeoTIFF. We use a helper function defined earlier, `naip_get_tile`, to check that the tile lies entirely inside the county shapefile and, if so, return the extracted image. Images are sorted into directories based on land use class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def extract_tiles(tiles, dest_folder, filename_base):\n",
    "    ''' Coordinates saving tile data, including extracted images and CSV descriptions '''\n",
    "    my_region_id = uuid.uuid4()\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "        \n",
    "    tile_descriptions = []\n",
    "    i = 0\n",
    "    while i < len(tiles):\n",
    "        tile = tiles[i]\n",
    "        tile_image = get_naip_tile(tile.bounds, tile_size)\n",
    "        i += 1\n",
    "        if (tile_image is None):\n",
    "            continue  # tile did not lie entirely within the county boundary (at least partially blank)\n",
    "        my_directory = os.path.join(dest_folder, '{:02d}'.format(tile.label))\n",
    "        my_filename = os.path.join(my_directory, '{}_{}.png'.format(filename_base, i))\n",
    "        if not os.path.exists(my_directory):\n",
    "            os.makedirs(my_directory)\n",
    "        tile_image.save(my_filename, 'PNG')\n",
    "    return\n",
    "\n",
    "output_dir = naip_dir  # if analyzing multiple counties, we recommending saving to the same output directory\n",
    "extract_tiles(tiles, output_dir, filename_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"combine\"></a>\n",
    "### Combining similar labels\n",
    "\n",
    "Some land use classes are difficult to distinguish in the visible spectrum using \"leaf-on\" imagery. For example, in the images extracted above, you are probably unable to distinguish the types of forest/woody wetland, or separate \"lightly developed\" from \"moderately developed\" land. For our use case, we found it acceptable to combine similar land use types into larger categories. If you choose this route, you may elect to combine the image folders after extraction, or increase the number of usable images by incorporating your grouping into the `find_tiles_with_consistent_labels()` function.\n",
    "\n",
    "We combined the land use types as follows:\n",
    "- **Developed**: 21, 22, 23, 24\n",
    "- **Water/Wetlands**: 11, 12, 95 (note: our dataset did not include any images of 12)\n",
    "- **Forest**: 41, 42, 43, 90\n",
    "- **Barren**: 31\n",
    "- **Shrubland**: 51, 52 (note: our dataset did not include any examples of 51)\n",
    "- **Grassland**: 71, 72, 73, 74 (note: our dataset did not include any examples of 72, 73, 74)\n",
    "- **Cultivated**: 81, 82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"partition\"></a>\n",
    "### Dataset partitioning\n",
    "\n",
    "We chose to divide images into training and evaluation datasets at the county level. This partitioning imposes a realistic dissimilarity between images in each set, which in term contributes to a more accurate estimate of model performance. Other important considerations during partitioning include maintaining similar class compositions in training vs. evaluation sets; balancing classes within each dataset (through pruning if necessary); and including diverse examples spanning the range of images likely to be encountered by the trained model after deployment.\n",
    "\n",
    "After combining labels and partitioning, our images were grouped by subfolders (named 0-6) in the folder `E:\\combined\\train_subsampled`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prep\"></a>\n",
    "## Dataset preparation for deep learning\n",
    "\n",
    "Above, we produced raw PNG images sorted into folders by label. We now produce the supporting input files that our training scripts require. Note that we do not need to produce these supporting files for the evaluation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cntk\"></a>\n",
    "### Cognitive Toolkit (CNTK)\n",
    "\n",
    "Our CNTK training script expects a tab-delimited \"map\" file that includes the full filepath and label for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def describe_image(filename, label, map_file):\n",
    "    map_file.write('{}\\t{}\\n'.format(filename, label))\n",
    "    return\n",
    "    \n",
    "image_dir = 'E:\\\\combined\\\\train_subsample'\n",
    "\n",
    "with open(os.path.join(image_dir, 'map.txt'), 'w') as map_file:\n",
    "    with open(os.path.join(image_dir, 'mean.txt'), 'w') as mean_file:\n",
    "        for label in os.listdir(image_dir):\n",
    "            my_dir = os.path.join(image_dir, label)\n",
    "            if not os.path.isdir(my_dir):\n",
    "                continue\n",
    "            for filename in os.listdir(my_dir):\n",
    "                describe_image(filename=os.path.join(my_dir, filename), label=label, map_file=map_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tf\"></a>\n",
    "### TensorFlow\n",
    "\n",
    "We made use of the [`tf-slim` API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) for Tensorflow, which provides pre-trained ResNet models and helpful scripts for retraining and scoring. Below, we convert our raw PNG images to the [TFRecords](https://www.tensorflow.org/how_tos/reading_data/#file_formats) files that those scripts expect as input. (Our evaluation images will be scored on Spark without conversion to TFRecord format.) We also create a `labels.txt` file mapping the folder names to integer labels, and a `dataset_split_info.csv` file describing the images assigned to the training set.\n",
    "\n",
    "The following code was modified from the [Tensorflow models repo's slim subdirectory](https://github.com/tensorflow/models/tree/master/slim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Original Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "# Modified 2017 by Microsoft Corporation.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "np.random.seed(5318)\n",
    "\n",
    "class ImageReader(object):\n",
    "    def __init__(self):\n",
    "        # Initializes function that decodes RGB JPEG data.\n",
    "        self._decode_png_data = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_png = tf.image.decode_png(self._decode_png_data, channels=3)\n",
    "\n",
    "    def read_image_dims(self, sess, image_data):\n",
    "        image = self.decode_png(sess, image_data)\n",
    "        return image.shape[0], image.shape[1]\n",
    "\n",
    "    def decode_png(self, sess, image_data):\n",
    "        image = sess.run(self._decode_png,\n",
    "                         feed_dict={self._decode_png_data: image_data})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image\n",
    "    \n",
    "def image_to_tfexample(image_data, image_format, height, width, class_id):\n",
    "    return tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_data])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
    "        'image/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=[class_id])),\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "    }))\n",
    "\n",
    "def find_and_split_images(image_dir, validation_fraction=0.2):\n",
    "    class_names = []\n",
    "    training_filenames = []\n",
    "    validation_filenames = []\n",
    "    \n",
    "    for folder in os.listdir(image_dir):\n",
    "        folder_path = os.path.join(image_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        ''' This is a new directory/label -- consider all images inside it '''\n",
    "        class_names.append(folder)\n",
    "        my_filenames = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            my_filenames.append(os.path.join(folder_path, filename))\n",
    "        n_validation = int(np.ceil(validation_fraction * len(my_filenames)))\n",
    "        validation_filenames.extend(my_filenames[:n_validation])\n",
    "        training_filenames.extend(my_filenames[n_validation:])\n",
    "    print('Found {} training and {} validation images'.format(len(training_filenames),\n",
    "                                                              len(validation_filenames)))\n",
    "    return(sorted(class_names), training_filenames, validation_filenames)\n",
    "    \n",
    "def write_dataset(dataset_name, split_name, my_filenames, class_names_to_ids, image_dir, n_shards=5):\n",
    "    num_per_shard = int(np.ceil(len(my_filenames) / n_shards))\n",
    "    records = []\n",
    "    with tf.Graph().as_default():\n",
    "        image_reader = ImageReader()\n",
    "        with tf.Session('') as sess:\n",
    "            for shard_idx in range(n_shards):\n",
    "                shard_filename = os.path.join(image_dir,\n",
    "                                              '{}_{}_{:05d}-of-{:05d}.tfrecord'.format(dataset_name,\n",
    "                                                                                       split_name,\n",
    "                                                                                       shard_idx+1,\n",
    "                                                                                       n_shards))\n",
    "                with tf.python_io.TFRecordWriter(shard_filename) as tfrecord_writer:\n",
    "                    for image_idx in range(num_per_shard * shard_idx,\n",
    "                                           min(num_per_shard * (shard_idx+1), len(my_filenames))):\n",
    "                        with open(my_filenames[image_idx], 'rb') as f:\n",
    "                            image_data = f.read()\n",
    "                        # Getting some sort of early EOF error with this version on Windows Server 2012:\n",
    "                        # image_data = tf.gfile.FastGFile(my_filenames[image_idx], 'r').read()\n",
    "                        height, width = image_reader.read_image_dims(sess, image_data)\n",
    "                        class_name = os.path.basename(os.path.dirname(my_filenames[image_idx]))\n",
    "                        class_id = class_names_to_ids[class_name]\n",
    "                        example = image_to_tfexample(image_data, b'png', height, width, class_id)\n",
    "                        tfrecord_writer.write(example.SerializeToString())\n",
    "                        records.append([dataset_name, split_name, my_filenames[image_idx], shard_idx,\n",
    "                                        image_idx, class_name, class_id])\n",
    "    df = pd.DataFrame(records, columns=['dataset_name', 'split_name', 'filename', 'shard_idx', 'image_idx',\n",
    "                                        'class_name', 'class_id'])\n",
    "    return(df)\n",
    " \n",
    "image_dir = 'E:\\\\combined\\\\train_subsample'\n",
    "\n",
    "# Our validation set has already been created and resides in a separate folder.\n",
    "# Assign all of the images in image_dir to the training set.\n",
    "class_names, training_filenames, _ = find_and_split_images(image_dir, 0.0)\n",
    "training_filenames = np.random.permutation(training_filenames)\n",
    "class_names_to_ids = dict(zip(class_names, list(range(len(class_names)))))\n",
    "df = write_dataset('aerial', 'train', training_filenames, class_names_to_ids, image_dir, n_shards=50)\n",
    "df.to_csv(os.path.join(image_dir, 'dataset_split_info.csv'), index=False)\n",
    "\n",
    "with open(os.path.join(image_dir, 'labels.txt'), 'w') as f:\n",
    "    for i in range(len(class_names)):\n",
    "        f.write('{0}:{0}\\n'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"transfer\"></a>\n",
    "\n",
    "## Data Transfer\n",
    "\n",
    "After generating the training and evaluation input data, we recommend creating a backup (and potentially transferring to a more powerful computer or VM for model training). We chose to transfer our data to an [Azure Blob Storage](https://docs.microsoft.com/en-us/azure/storage/storage-create-storage-account) account using the command line tool [AzCopy](https://docs.microsoft.com/en-us/azure/storage/storage-use-azcopy) (but see [Azure Storage Explorer](http://storageexplorer.com/) for a GUI-based alternative).\n",
    "\n",
    "Don't forget to delete any unneeded resources once you have completed the tutorial!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Please see the next notebook in the [Embarrassingly Parallel Image Classification](https://github.com/Azure/Embarrassingly-Parallel-Image-Classification) repository, [Model Training](https://github.com/Azure/Embarrassingly-Parallel-Image-Classification/blob/master/model_training.ipynb), for information on training or retraining a deep neural network for image classification."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
